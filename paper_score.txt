Certifiable Distributional Robustness with Principled Adversarial Training 9.00
On the Convergence of Adam and Beyond 8.33
Deep Mean Field Games for Learning Optimal Behavior Policy of Large Populations 8.33
Emergence of grid-like representations by training recurrent neural networks to perform spatial localization 8.33
Multi-Scale Dense Networks for Resource Efficient Image Classification 8.33
Spherical CNNs 8.00
Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments 8.00
i-RevNet: Deep Invertible Networks 8.00
Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions 8.00
Wasserstein Auto-Encoders 8.00
Learning to Represent Programs with Graphs 8.00
A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks 7.33
Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent 7.33
Eigenoption Discovery through the Deep Successor Representation 7.33
Learning how to explain neural networks: PatternNet and PatternAttribution 7.33
Neural Sketch Learning for Conditional Program Generation 7.33
Learning Differentially Private Recurrent Language Models 7.33
Learning One-hidden-layer Neural Networks with Landscape Design 7.33
Modular Continual Learning in a Unified Visual Environment 7.33
Neural Speed Reading via Skim-RNN 7.33
A DIRT-T Approach to Unsupervised Domain Adaptation 7.33
Distributed Prioritized Experience Replay 7.33
Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models 7.33
Synthetic and Natural Noise Both Break Neural Machine Translation 7.33
Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection 7.33
Skip Connections Eliminate Singularities 7.33
CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training 7.33
Spectral Normalization for Generative Adversarial Networks 7.33
AmbientGAN : Generative models from lossy measurements 7.33
Unsupervised Machine Translation Using Monolingual Corpora Only 7.33
GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING 7.25
Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning 7.00
Learning from Between-class Examples for Deep Sound Recognition 7.00
Lifelong Learning with Dynamically Expandable Networks 7.00
Active Neural Localization 7.00
Learning Wasserstein Embeddings 7.00
Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality 7.00
Backpropagation through the Void: Optimizing control variates for black-box gradient estimation 7.00
A Deep Reinforced Model for Abstractive Summarization 7.00
On the importance of single directions for generalization 7.00
Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling 7.00
Smooth Loss Functions for Deep Top-k Classification 7.00
A Scalable Laplace Approximation for Neural Networks 7.00
Learning Latent Permutations with Gumbel-Sinkhorn Networks 7.00
Learning to Teach 7.00
Understanding Short-Horizon Bias in Stochastic Meta-Optimization 7.00
Alternating Multi-bit Quantization for Recurrent Neural Networks 7.00
Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation 7.00
Compressing Word Embeddings via Deep Compositional Code Learning 7.00
DCN+: Mixed Objective And Deep Residual Coattention for Question Answering 7.00
Deep Learning with Logged Bandit Feedback 7.00
Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play 7.00
Fixing Weight Decay Regularization in Adam 7.00
Generalizing Hamiltonian Monte Carlo with Neural Networks 7.00
Efficient Sparse-Winograd Convolutional Neural Networks 7.00
Zero-Shot Visual Imitation 7.00
Simulating Action Dynamics with Neural Process Networks 7.00
PixelNN: Example-based Image Synthesis 7.00
Relational Neural Expectation Maximization 7.00
Deep Learning as a Mixed Convex-Combinatorial Optimization Problem 7.00
Variational Network Quantization 7.00
Neural Language Modeling by Jointly Learning Syntax and Lexicon 7.00
Hyperparameter optimization: a spectral approach 7.00
A Neural Representation of Sketch Drawings 7.00
Multi-level Residual Networks from Dynamical Systems View 7.00
Generative Models of Visually Grounded Imagination 7.00
Variance Reduction for Policy Gradient Methods with Action-Dependent Baselines 7.00
Spatially Transformed Adversarial Examples 7.00
When is a Convolutional Filter Easy to Learn? 7.00
 Neural Map: Structured Memory for Deep Reinforcement Learning 7.00
Regularizing and Optimizing LSTM Language Models 7.00
Variational image compression with a scale hyperprior 7.00
Certified Defenses against Adversarial Examples  7.00
SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE 7.00
PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples 6.67
Parallelizing Linear Recurrent Neural Nets Over Sequence Length 6.67
Towards Synthesizing Complex Programs From Input-Output Examples 6.67
Attacking Binarized Neural Networks 6.67
Improving GANs Using Optimal Transport 6.67
Polar Transformer Networks 6.67
Interpretable and Pedagogical Examples 6.67
On the Expressive Power of Overlapping Architectures of Deep Learning 6.67
Convolving DNA using two-dimensional Hilbert curve representations 6.67
Training GANs with Optimism 6.67
Towards better understanding of gradient-based attribution methods for Deep Neural Networks 6.67
Wavelet Pooling for Convolutional Neural Networks 6.67
PDE-Net: Learning PDEs from Data 6.67
Distributional Policy Gradients 6.67
Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy 6.67
The Kanerva Machine: A Generative Distributed Memory 6.67
Global Optimality Conditions for Deep Neural Networks 6.67
Mixed Precision Training 6.67
Active Learning for Convolutional Neural Networks: A Core-Set Approach 6.67
Non-Autoregressive Neural Machine Translation 6.67
On the State of the Art of Evaluation in Neural Language Models 6.67
Model-Ensemble Trust-Region Policy Optimization 6.67
A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs 6.67
The Implicit Bias of Gradient Descent on Separable Data 6.67
NerveNet: Learning Structured Policy with Graph Neural Networks 6.67
SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data 6.67
SEARNN: Training RNNs with global-local losses 6.67
Critical Percolation as a Framework to Analyze the Training of Deep Networks 6.67
On the insufficiency of existing momentum schemes for Stochastic Optimization 6.67
Stochastic Variational Video Prediction 6.67
Unbiased Online Recurrent Optimization 6.67
Learning Weighted Representations for Generalization Across Designs 6.67
Activation Maximization Generative Adversarial Nets 6.67
Emergent Translation in Multi-Agent Communication 6.67
Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge 6.67
Loss-aware Weight Quantization of Deep Networks 6.67
Deep Gaussian Embedding of Graphs:  Unsupervised Inductive Learning via Ranking 6.67
Adversarial Dropout Regularization 6.67
Towards Neural Phrase-based Machine Translation 6.67
Hierarchical Representations for Efficient Architecture Search 6.67
Critical Points of Neural Networks: Analytical Forms and Landscape Properties 6.67
Learning to cluster in order to Transfer across domains and tasks 6.67
Emergent Communication in a Multi-Modal, Multi-Step Referential Game 6.67
LEARNING A GENERATIVE MODEL FOR VALIDITY IN COMPLEX DISCRETE STRUCTURES 6.67
Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis 6.67
On the Information Bottleneck Theory of Deep Learning 6.67
Emergence of Linguistic Communication from  Referential Games with Symbolic and Pixel Input 6.67
Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering 6.67
Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks 6.67
Temporally Efficient Deep Learning with Spikes 6.67
Can recurrent neural networks warp time? 6.67
Learning an Embedding Space for Transferable Robot Skills 6.67
TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS 6.67
Online Learning Rate Adaptation with Hypergradient Descent 6.67
Parameter Space Noise for Exploration 6.67
Sobolev GAN 6.50
Large Scale Optimal Transport and Mapping Estimation 6.50
FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling 6.50
Learning Approximate Inference Networks for Structured Prediction 6.33
The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning 6.33
Distribution Regression Network 6.33
Understanding Deep Neural Networks with Rectified Linear Units 6.33
The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings 6.33
Scalable Private Learning with PATE 6.33
Memory-based Parameter Adaptation 6.33
Learning Sparse Neural Networks through L_0 Regularization 6.33
Learning Dynamic State Abstractions for Model-Based Reinforcement Learning 6.33
cGANs with Projection Discriminator 6.33
Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches 6.33
FearNet: Brain-Inspired Model for Incremental Learning 6.33
Learning Intrinsic Sparse Structures within Long Short-Term Memory 6.33
Interactive Grounded Language Acquisition and Generalization in a 2D World 6.33
Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks 6.33
Communication Algorithms via Deep Learning 6.33
Self-ensembling for visual domain adaptation 6.33
Depthwise Separable Convolutions for Neural Machine Translation 6.33
On the inductive bias of stochastic gradient descent 6.33
Stochastic activation pruning for robust adversarial defense 6.33
Training and Inference with Integers in Deep Neural Networks 6.33
Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models 6.33
Weighted Transformer Network for Machine Translation 6.33
Learning Robust Rewards with Adverserial Inverse Reinforcement Learning 6.33
Recasting Gradient-Based Meta-Learning as Hierarchical Bayes 6.33
WRPN: Wide Reduced-Precision Networks 6.33
Don't Decay the Learning Rate, Increase the Batch Size 6.33
Decoupling the Layers in Residual Networks 6.33
Few-Shot Learning with Graph Neural Networks 6.33
A Simple Neural Attentive Meta-Learner 6.33
The power of deeper networks for expressing natural functions 6.33
THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS 6.33
FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension 6.33
Deep Rewiring: Training very sparse deep networks 6.33
Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference 6.33
Memory Architectures in Recurrent Neural Network Language Models 6.33
Countering Adversarial Images using Input Transformations 6.33
HexaConv 6.33
On Unifying Deep Generative Models 6.33
Compositional Attention Networks for Machine Reasoning 6.33
Stabilizing Adversarial Nets with Prediction Methods 6.33
HALLUCINATING BRAINS WITH ARTIFICIAL BRAINS 6.33
Assessing the scalability of biologically-motivated deep learning algorithms and architectures 6.33
Initialization matters: Orthogonal Predictive State Recurrent Neural Networks 6.33
Universal Agent for Disentangling Environments and Tasks 6.33
Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training 6.33
Breaking the Softmax Bottleneck: A High-Rank RNN Language Model 6.33
Word translation without parallel data 6.33
Mitigating Adversarial Effects Through Randomization 6.33
Divide and Conquer Networks 6.33
Attention-based Graph Neural Network for Semi-supervised Learning 6.33
Unsupervised Cipher Cracking Using Discrete GANs 6.33
DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension 6.33
Thinking like a machine — generating visual rationales through latent space optimization 6.33
Emergent Complexity via Multi-Agent Competition 6.33
Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields 6.33
On the Use of Word Embeddings Alone to Represent Natural Language Sequences 6.33
An efficient framework for learning sentence representations 6.33
Semantically Decomposing the Latent Spaces of Generative Adversarial Networks 6.33
Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning 6.33
Twin Networks: Matching the Future for Sequence Generation 6.33
VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop 6.33
Memory Augmented Control Networks 6.33
Learning Representations and Generative Models for 3D Point Clouds 6.33
Synthesizing Robust Adversarial Examples 6.33
An empirical study on evaluation metrics of generative adversarial networks 6.25
Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design 6.25
Thermometer Encoding: One Hot Way To Resist Adversarial Examples 6.00
Ask the Right Questions: Active Question Reformulation with Reinforcement Learning 6.00
GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders 6.00
Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control 6.00
Espresso: Efficient Forward Propagation for Binary Deep Neural Networks 6.00
Reinforcement Learning Algorithm Selection 6.00
Contextual Explanation Networks 6.00
Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis 6.00
Bag of region embeddings via local context unit for text classification 6.00
RESIDUAL LOSS PREDICTION: REINFORCEMENT LEARNING WITH NO INCREMENTAL FEEDBACK 6.00
DLVM: A modern compiler infrastructure for deep learning systems 6.00
Kernel Implicit Variational Inference 6.00
Learning Discrete Weights Using the Local Reparameterization Trick 6.00
Gradient Estimators for Implicit Models 6.00
ON MODELING HIERARCHICAL DATA VIA ENCAPSULATION OF PROBABILITY DENSITIES 6.00
Data Augmentation Generative Adversarial Networks 6.00
A Hierarchical Model for Device Placement 6.00
Model compression via distillation and quantization 6.00
Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs 6.00
Analyzing GANs with Generative Scattering Networks 6.00
AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks 6.00
Towards Deep Learning Models Resistant to Adversarial Attacks 6.00
CyCADA: Cycle-Consistent Adversarial Domain Adaptation 6.00
Fidelity-Weighted Learning 6.00
Monotonic Chunkwise Attention 6.00
Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models 6.00
Recurrent Neural Networks with Top-k Gains for Session-based Recommendations 6.00
Emergent Communication through Negotiation 6.00
GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets 6.00
Deep Lipschitz networks and Dudley GANs 6.00
Kronecker-factored Curvature Approximations for Recurrent Neural Networks 6.00
Divide and Conquer Reinforcement Learning 6.00
Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound 6.00
Transformation Autoregressive Networks 6.00
Training RNNs as Fast as CNNs 6.00
Few-Shot Learning with Variational Homoencoders 6.00
Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples 6.00
An Online Learning Approach to Generative Adversarial Networks 6.00
Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering 6.00
Parameterized Hierarchical Procedures for Neural Programming 6.00
Adversarially Regularized Autoencoders 6.00
Semantic Interpolation in Implicit Models 6.00
Gaussian Process Behaviour in Wide Deep Neural Networks 6.00
UCB EXPLORATION VIA Q-ENSEMBLES 6.00
Residual Connections Encourage Iterative Inference 6.00
Toward learning better metrics for sequence generation training with policy gradient 6.00
Variational Continual Learning 6.00
Maximum a Posteriori Policy Optimisation 6.00
Boosting the Actor with Dual Critic 6.00
Multi-Mention Learning for Reading Comprehension with Neural Cascades 6.00
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning 6.00
Variational Bi-LSTMs 6.00
Interpretable Counting for Visual Question Answering 6.00
Ensemble Adversarial Training: Attacks and Defenses 6.00
Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples 6.00
Coupled Ensembles of Neural Networks 6.00
MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank 6.00
Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks 6.00
Hierarchical Subtask Discovery with Non-Negative Matrix Factorization 6.00
Training wide residual networks for deployment using a single bit for each weight 6.00
Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step 6.00
Variational Message Passing with Structured Inference Networks 6.00
Robustness of Classifiers to Universal Perturbations: A Geometric Perspective 6.00
Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts 6.00
Unsupervised Representation Learning by Predicting Image Rotations 6.00
Orthogonal Recurrent Neural Networks with Scaled Cayley Transform 6.00
Mixed Precision Training of Convolutional Neural Networks using Integer Operations 6.00
Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning 6.00
Synthesizing realistic neural population activity patterns using semi-convolutional GANs 6.00
All-but-the-Top: Simple and Effective Postprocessing for Word Representations 6.00
Large scale distributed neural network training through online distillation 6.00
Distributional Adversarial Networks 6.00
Unsupervised Neural Machine Translation 6.00
Multi-Agent Compositional Communication Learning from Raw Visual Input 6.00
N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning 6.00
Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio 6.00
Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration 6.00
SMASH: One-Shot Model Architecture Search through HyperNetworks 6.00
Towards Image Understanding from Deep Compression Without Decoding 6.00
Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks 6.00
CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS 6.00
DORA The Explorer: Directed Outreaching Reinforcement Action-Selection 6.00
Learning a neural response metric for retinal prosthesis 6.00
Deep Voice 3: 2000-Speaker Neural Text-to-Speech 6.00
Whitening Black-Box Neural Networks 6.00
Learning From Noisy Singly-labeled Data 6.00
Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy 6.00
Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting 6.00
Detecting Statistical Interactions from Neural Network Weights 6.00
Noisy Networks For Exploration 6.00
Consequentialist conditional cooperation in social dilemmas with imperfect information 6.00
Learning to Multi-Task by Active Sampling 6.00
Multiple Source Domain Adaptation with Adversarial Learning 5.75
UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT 5.67
WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling 5.67
Jiffy: A Convolutional Approach to Learning Time Series Similarity 5.67
Stable Distribution Alignment Using the Dual of the Adversarial Distance 5.67
SpectralNet: Spectral Clustering using Deep Neural Networks 5.67
Multi-Task Learning for Document Ranking and Query Suggestion 5.67
Empirical Risk Landscape Analysis for Understanding Deep Neural Networks 5.67
Not-So-Random Features 5.67
Massively Parallel Hyperparameter Tuning 5.67
Principled Hybrids of Generative and Discriminative Domain Adaptation 5.67
Generating Natural Adversarial Examples 5.67
Building effective deep neural network architectures one feature at a time 5.67
Efficient Exploration through Bayesian   Deep Q-Networks 5.67
Learning Priors for Adversarial Autoencoders 5.67
Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy 5.67
Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip 5.67
VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS 5.67
Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks 5.67
Measuring the Intrinsic Dimension of Objective Landscapes 5.67
Large Scale Multi-Domain Multi-Task Learning with MultiModel 5.67
Predicting Floor-Level for 911 Calls with Recurrent Neural Networks and Smartphone Sensor Data 5.67
Quadrature-based features for kernel approximation 5.67
Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers 5.67
An information-theoretic analysis of deep latent-variable models 5.67
Deep contextualized word representations 5.67
Do GANs learn the distribution? Some Theory and Empirics 5.67
Benefits of Depth for Long-Term Memory of Recurrent Networks 5.67
GraphGAN: Generating Graphs via Random Walks 5.67
The High-Dimensional Geometry of Binary Neural Networks 5.67
Deep Active Learning for Named Entity Recognition 5.67
Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design 5.67
Byte-Level Recursive Convolutional Auto-Encoder for Text 5.67
Guide Actor-Critic for Continuous Control 5.67
Nuclear p-norms for large tensor completion 5.67
Learning to Optimize Neural Nets 5.67
Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks 5.67
Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization 5.67
Network Iterative Learning for Dynamic Deep Neural Networks via Morphism 5.67
Convolutional Sequence Modeling Revisited 5.67
Learning Deep Models: Critical Points and Local Openness 5.67
Faster Reinforcement Learning with Expert State Sequences 5.67
A Bayesian Perspective on Generalization and Stochastic Gradient Descent 5.67
Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm 5.67
Deep Complex Networks 5.67
Federated Learning: Strategies for Improving Communication Efficiency 5.67
Learning to diagnose from scratch by exploiting dependencies among labels 5.67
Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement 5.67
Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs 5.67
Automatic Goal Generation for Reinforcement Learning Agents 5.67
Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction 5.67
Can Neural Networks Understand Logical Entailment? 5.67
Log-DenseNet: How to Sparsify a DenseNet 5.67
Bayesian Recurrent Neural Networks 5.67
mixup: Beyond Empirical Risk Minimization 5.67
Quantitatively Evaluating GANs With Divergences Proposed for Training 5.67
Expressive power of recurrent neural networks 5.67
Graph Attention Networks 5.67
Deep Neural Networks as Gaussian Processes 5.67
Building Generalizable Agents with a Realistic and Rich 3D Environment 5.67
Cascade Adversarial Machine Learning Regularized with a Unified Embedding 5.67
Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions 5.67
Demystifying MMD GANs 5.67
Large Margin Neural Language Models 5.67
Learning to Compute Word Embeddings On the Fly 5.67
Kronecker Recurrent Units 5.67
Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum 5.67
Multiscale Hidden Markov Models For Covariance Prediction 5.67
Block-Sparse Recurrent Neural Networks 5.67
Implicit Causal Models for Genome-wide Association Studies 5.67
Adaptive Quanization of Neural Networks 5.67
Bias-Variance Decomposition for Boltzmann Machines 5.67
Genetic Policy Optimization 5.67
TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning 5.67
Learning Deep Generative Models of Graphs 5.67
 Explicit Induction Bias for Transfer Learning with Convolutional Networks 5.67
Deep Generative Dual Memory Network for Continual Learning 5.67
SCAN: Learning Hierarchical Compositional Visual Concepts 5.67
Latent Space Oddity: on the Curvature of Deep Generative Models 5.67
Code Synthesis with Priority Queue Training 5.67
MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$ 5.67
Decision Boundary Analysis of Adversarial Examples 5.67
Fix your classifier: the marginal value of training the last weight layer 5.67
Matrix capsules with EM routing 5.67
Feature Incay for Representation Regularization 5.67
Improving diversity in Generative adversarial networks by encouraging discriminator representation entropy 5.67
Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach 5.67
“Style” Transfer for Musical Audio Using Multiple Time-Frequency Representations 5.67
Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks 5.67
Learning Awareness Models 5.67
Stochastic Hyperparameter Optimization through Hypernetworks 5.67
Language Style Transfer from Non-Parallel Text with Arbitrary Styles 5.67
Theoretical properties of the global optimizer of two-layer Neural Network 5.67
Learning temporal evolution of probability distribution with Recurrent Neural Network 5.67
Automatically Inferring Data Quality for Spatiotemporal Forecasting 5.67
Variance-based Gradient Compression for Efficient Distributed Deep Learning 5.67
Temporal Difference Model Learning Model-Free Deep RL for Model-Based Control 5.67
Exponentially vanishing sub-optimal local minima in multilayer neural networks 5.67
Meta-Learning for Semi-Supervised Few-Shot Classification 5.67
Flexible Prior Distributions for Deep Generative Models 5.67
Scheduled Learning with Declining Diversity and Incremental Difficulty 5.67
Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning 5.67
Multi-View Data Generation Without View Supervision 5.67
Learn to Pay Attention 5.67
Improving the Improved Training of Wasserstein GANs 5.67
Automatic Parameter Tying in Neural Networks 5.67
Learning Gaussian Policies from Smoothed Action Value Functions 5.67
Lifelong Generative Modeling 5.67
Progressive Growing of GANs for Improved Quality, Stability, and Variation 5.67
Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning 5.67
Deep Mean Field Theory: Variance and Width Variation by Layer as Methods to Control Gradient Explosion 5.67
MGAN: Training Generative Adversarial Nets with Multiple Generators 5.67
Learning Efficient Tensor Representations with Ring Structure Networks 5.50
Continuous Convolutional Neural Networks for Image Classification 5.50
The loss surface and expressivity of deep convolutional neural networks 5.50
Correcting Nuisance Variation using Wasserstein Distance 5.33
A framework for the quantitative evaluation of disentangled representations 5.33
Modeling Latent Attention Within Neural Networks 5.33
Identifying Analogies Across Domains 5.33
Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation 5.33
Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks 5.33
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks 5.33
Learning to search with MCTSnets 5.33
Natural Language Inference over Interaction Space 5.33
Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning 5.33
MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES 5.33
SHADE: SHAnnon DEcay Information-Based Regularization for Deep Learning 5.33
Learning Latent Representations in Neural Networks for Unsupervised Clustering through Pseudo Supervision and Graph-based Activity Regularization 5.33
Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML 5.33
Phase Conductor on Multi-layered Attentions for Machine Comprehension 5.33
Capturing Human Category Representations by Sampling in Deep Feature Spaces 5.33
Learning to Write by Learning the Objective 5.33
Auxiliary Guided Autoregressive Variational Autoencoders 5.33
The Information-Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Modeling 5.33
Unbiasing Truncated Backpropagation Through Time 5.33
Imitation Learning from Visual Data with Multiple Intentions 5.33
Accelerating Neural Architecture Search using Performance Prediction 5.33
Achieving Strong Regularization for Deep Neural Networks 5.33
Small Coresets to Represent Large Training Data for Support Vector Machines 5.33
Learning to Infer 5.33
Semi-Supervised Learning via New Deep Network Inversion 5.33
Intriguing Properties of Adversarial Examples 5.33
Exploring the Hidden Dimension in Accelerating Convolutional Neural Networks 5.33
On the difference between building and extracting patterns: a causal analysis of deep generative models. 5.33
Dynamic Evaluation of Neural Sequence Models 5.33
No Spurious Local Minima in a Two Hidden Unit ReLU Network 5.33
Learning Audio Features for Singer Identification and Embedding 5.33
Semantic Code Repair using Neuro-Symbolic Transformation Networks 5.33
Deep Learning Inferences with Hybrid Homomorphic Encryption 5.33
DropMax: Adaptive Stochastic Softmax 5.33
DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer 5.33
Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations 5.33
Value Propagation Networks 5.33
Learning Independent Causal Mechanisms 5.33
Fast and Accurate Text Classification: Skimming, Rereading and Early Stopping 5.33
Multi-Task Learning by Deep Collaboration and Application in Facial Landmark Detection 5.33
Network of Graph Convolutional Networks \\ Trained on Random Walks 5.33
Predict Responsibly: Increasing Fairness by Learning to Defer 5.33
Sparse-Complementary Convolution for Efficient Model Utilization on CNNs 5.33
Challenges in Disentangling Independent Factors of Variation 5.33
Lifelong Learning by Adjusting Priors 5.33
Discrete Sequential Prediction of Continuous Actions for Deep RL 5.33
Can Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games? 5.33
Avoiding Catastrophic States with Intrinsic Fear 5.33
The Cramer Distance as a Solution to Biased Wasserstein Gradients 5.33
FigureQA: An Annotated Figure Dataset for Visual Reasoning 5.33
Training Autoencoders by Alternating Minimization 5.33
On the limitations of first order approximation in GAN dynamics 5.33
A Flexible Approach to Automated RNN Architecture Generation 5.33
Gradients explode - Deep Networks are shallow - ResNet explained 5.33
LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING 5.33
Composable Planning with Attributes 5.33
ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions 5.33
Regret Minimization for Partially Observable Deep Reinforcement Learning 5.33
Dynamic Integration of Background Knowledge in Neural NLU Systems 5.33
Understanding Grounded Language Learning Agents 5.33
Searching for Activation Functions 5.33
Towards Binary-Valued Gates for Robust LSTM Training  5.33
Loss Functions for Multiset Prediction 5.33
Weightless: Lossy Weight Encoding For Deep Neural Network Compression 5.33
Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior 5.33
BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS 5.33
Adaptive Dropout with Rademacher Complexity Regularization 5.33
Proximal Backpropagation 5.33
Boundary Seeking GANs 5.33
A Painless Attention Mechanism for Convolutional Neural Networks 5.33
Better Generalization by Efficient Trust Region Method 5.33
LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION 5.33
Fast and Accurate Inference with Adaptive Ensemble Prediction for Deep Networks 5.33
Generating Wikipedia by Summarizing Long Sequences 5.33
A Spectral Approach to Generalization and Optimization in Neural Networks 5.33
Graph Partition Neural Networks for Semi-Supervised Classification 5.33
Deterministic Policy Imitation Gradient Algorithm 5.33
Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization 5.33
Character Level Based Detection of DGA Domain Names 5.33
Fraternal Dropout 5.33
SIC-GAN: A Self-Improving Collaborative GAN for Decoding Variational RNNs 5.33
DNN Feature Map Compression using Learned Representation over GF(2) 5.33
Understanding image motion with group representations  5.33
Learning non-linear transform with discriminative and minimum information loss priors 5.33
Transfer Learning to Learn with Multitask Neural Model Search 5.33
Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning 5.33
FAST READING COMPREHENSION WITH CONVNETS 5.33
Gaussian Process Neurons 5.33
Trace norm regularization and faster inference for embedded speech recognition RNNs 5.33
Analyzing and Exploiting NARX Recurrent Neural Networks for Long-Term Dependencies 5.33
Towards Provable Control for Unknown Linear Dynamical Systems 5.33
Optimizing the Latent Space of Generative Networks 5.33
Fast and Accurate Reading Comprehension Without Recurrent Networks 5.33
Cheap DNN Pruning with Performance Guarantees  5.33
Optimal transport maps for distribution preserving operations on latent spaces of Generative Models 5.33
Inference Dissection in Variational Autoencoders 5.33
Neural Program Search: Solving Data Processing Tasks from Description and Examples 5.33
Learning Sparse Latent Representations with the Deep Copula Information Bottleneck 5.33
STRUCTURED ALIGNMENT NETWORKS 5.33
MACE: Structured Exploration via Deep Hierarchical Coordination 5.33
Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling 5.33
Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms 5.33
Generating Adversarial Examples with Adversarial Networks 5.33
Discrete-Valued Neural Networks Using Variational Inference 5.33
SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning 5.33
Trust-PCL: An Off-Policy Trust Region Method for Continuous Control 5.33
Dynamic Neural Program Embeddings for Program Repair 5.33
Some Considerations on Learning to Explore via Meta-Reinforcement Learning 5.33
A Deep Learning Approach for Survival Clustering without End-of-life Signals 5.33
Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning 5.33
Policy Gradient For Multidimensional Action Spaces: Action Sampling and Entropy Bonus 5.33
Baseline-corrected space-by-time non-negative matrix factorization for decoding single trial population spike trains 5.33
An Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems. 5.33
Large-scale Cloze Test Dataset Designed by Teachers 5.33
Stabilizing GAN Training with Multiple Random Projections 5.33
A Semantic Loss Function for Deep Learning with Symbolic Knowledge 5.33
AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT 5.33
On the Discrimination-Generalization Tradeoff in GANs 5.33
Global Convergence of Policy Gradient Methods for Linearized  Control Problems 5.33
Neural Compositional Denotational Semantics for Question Answering 5.33
Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks 5.33
Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach 5.33
Natural Language Inference with External Knowledge 5.25
Incremental Learning through Deep Adaptation 5.00
Syntax-Directed Variational Autoencoder for Structured Data 5.00
Neural Networks with Block Diagonal Inner Product Layers 5.00
Auto-Encoding Sequential Monte Carlo 5.00
Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs 5.00
Using Deep Reinforcement Learning to Generate Rationales for Molecules 5.00
Towards a Testable Notion of Generalization for Generative Adversarial Networks 5.00
Learning to Count Objects in Natural Images for Visual Question Answering 5.00
Unbiased scalable softmax optimization 5.00
WSNet: Learning Compact and Efficient Networks with Weight Sampling 5.00
Counterfactual Image Networks 5.00
Detecting Anomalies in Communication Packet Streams based on  Generative Adversarial Networks 5.00
From Information Bottleneck To Activation Norm Penalty 5.00
End-to-End Abnormality Detection in Medical Imaging 5.00
Tandem Blocks in Deep Convolutional Neural Networks 5.00
LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION 5.00
Continuous-Time Flows for Efficient Inference and Density Estimation 5.00
Ego-CNN: An Ego Network-based Representation of Graphs Detecting Critical Structures 5.00
Continuous-fidelity Bayesian Optimization with Knowledge Gradient 5.00
A Bayesian Nonparametric Topic Model with Variational Auto-Encoders 5.00
Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network 5.00
Realtime query completion via deep language models 5.00
Curiosity-driven Exploration by Bootstrapping Features 5.00
Censoring Representations with Multiple-Adversaries over Random Subspaces 5.00
Faster Discovery of Neural Architectures by Searching for Paths in a Large Model 5.00
Multimodal Sentiment Analysis To Explore the Structure of Emotions 5.00
Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings 5.00
DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images 5.00
Adaptive Memory Networks 5.00
Disentangled activations in deep networks 5.00
Discrete Autoencoders for Sequence Models 5.00
Investigating Human Priors for Playing Video Games 5.00
VOCABULARY-INFORMED VISUAL FEATURE AUGMENTATION FOR ONE-SHOT LEARNING 5.00
Ground-Truth Adversarial Examples 5.00
Variance Regularizing Adversarial Learning 5.00
Pixel Deconvolutional Networks 5.00
Novelty Detection with GAN 5.00
Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs 5.00
Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation 5.00
Bounding and Counting Linear Regions of Deep Neural Networks 5.00
Connectivity Learning in Multi-Branch Networks 5.00
Neural Task Graph Execution 5.00
INTERPRETATION OF NEURAL NETWORK IS FRAGILE 5.00
Rotational Unit of Memory  5.00
REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED 5.00
Adversarial Learning for Semi-Supervised Semantic Segmentation 5.00
Discovering the mechanics of hidden neurons 5.00
Semi-parametric topological memory for navigation 5.00
LSH Softmax: Sub-Linear Learning and Inference of the Softmax Layer in Deep Architectures 5.00
Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor 5.00
Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning 5.00
Long-term Forecasting using Tensor-Train RNNs 5.00
Latent Topic Conversational Models 5.00
Adversarial Policy Gradient for Alternating Markov Games 5.00
Reinforcement and Imitation Learning for Diverse Visuomotor Skills 5.00
Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier 5.00
Neural Clustering By Predicting And Copying Noise 5.00
Residual Gated Graph ConvNets 5.00
Learnability of Learned Neural Networks 5.00
Learning Graph Convolution Filters from Data Manifold 5.00
PACT: Parameterized Clipping Activation for Quantized Neural Networks 5.00
Hierarchical Adversarially Learned Inference 5.00
Universality, Robustness, and Detectability of Adversarial Perturbations under Adversarial Training 5.00
DNN Representations as Codewords: Manipulating Statistical Properties via Penalty Regularization 5.00
Learning Covariate-Specific Embeddings with Tensor Decompositions 5.00
On Convergence and Stability of GANs 5.00
Reinforcement Learning from Imperfect Demonstrations 5.00
Fast Node Embeddings: Learning Ego-Centric Representations 5.00
Hyperedge2vec: Distributed Representations for Hyperedges 5.00
Simple and efficient architecture search for Convolutional Neural Networks 5.00
Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks 5.00
Discriminative k-shot learning using probabilistic models 5.00
Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification 5.00
Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning 5.00
Topic-Based Question Generation 5.00
Bayesian Hypernetworks 5.00
DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING 5.00
Covariant Compositional Networks For Learning Graphs 5.00
To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression 5.00
Feature Map Variational Auto-Encoders 5.00
Sensitivity and Generalization in Neural Networks 5.00
Learning Approximate Closed-Loop Policies for Markov Potential Games 5.00
Neuron as an Agent 5.00
Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning 5.00
TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION 5.00
Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update 5.00
Data Augmentation by Pairing Samples for Images Classification 5.00
COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS 5.00
Generating Differentially Private Datasets Using GANs 5.00
Bayesian Time Series Forecasting with Change Point and Anomaly Detection 4.67
Few-shot learning with simplex 4.67
Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration 4.67
Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning  4.67
GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks 4.67
Normalized Direction-preserving Adam 4.67
Tree-to-tree Neural Networks for Program Translation 4.67
Predicting Multiple Actions for Stochastic Continuous Control 4.67
Three factors influencing minima in SGD 4.67
Egocentric Spatial Memory Network 4.67
UNSUPERVISED METRIC LEARNING VIA NONLINEAR FEATURE SPACE TRANSFORMATIONS 4.67
Kernel Graph Convolutional Neural Nets 4.67
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks 4.67
Interactive Boosting of Neural Networks for Small-sample Image Classification 4.67
PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE 4.67
Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks 4.67
Adversarial reading networks for machine comprehension 4.67
Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients 4.67
Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases 4.67
Shifting Mean Activation Towards Zero with Bipolar Activation Functions 4.67
Learning Parsimonious Deep Feed-forward Networks 4.67
Free energy-based reinforcement learning using a quantum processor 4.67
Feat2Vec:  Dense Vector Representation for Data with Features 4.67
On Optimality Conditions for Auto-Encoder Signal Recovery 4.67
Sensor Transformation Attention Networks 4.67
Unseen Class Discovery in Open-world Classification 4.67
Unleashing the Potential of CNNs for Interpretable Few-Shot Learning 4.67
Stochastic Training of Graph Convolutional Networks 4.67
Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus 4.67
Learning Independent Features with Adversarial Nets for Non-linear ICA 4.67
Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient 4.67
Deep Asymmetric Multi-task Feature Learning 4.67
Tensor Contraction & Regression Networks 4.67
Graph Classification with 2D Convolutional Neural Networks 4.67
Explaining the Decisions of Neural Networks with Latent Sympathetic Examples 4.67
Empirical Analysis of the Hessian of Over-Parametrized Neural Networks 4.67
Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network 4.67
Neighbor-encoder 4.67
YellowFin and the Art of Momentum Tuning 4.67
Deep Continuous Clustering 4.67
Anomaly Detection with Generative Adversarial Networks 4.67
META LEARNING SHARED HIERARCHIES 4.67
Unsupervised Deep Structure Learning by Recursive Dependency Analysis 4.67
Multitask learning of Multilingual Sentence Representations 4.67
Bayesian Embeddings for Long-Tailed Datasets 4.67
Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning 4.67
Autoregressive Convolutional Neural Networks for Asynchronous Time Series 4.67
Parametric Information Bottleneck to \\Optimize Stochastic Neural Networks 4.67
A Boon for Evaluating Architecture Performance 4.67
IVE-GAN: Invariant Encoding Generative Adversarial Networks 4.67
Model-based imitation learning from state trajectories 4.67
Overcoming the vanishing gradient problem in plain recurrent networks 4.67
On the regularization of Wasserstein GANs 4.67
Learning Non-Metric Visual Similarity for Image Retrieval 4.67
Linearly Constrained Weights: Resolving the Vanishing Gradient Problem by Reducing Angle Bias 4.67
Learning to select examples for program synthesis 4.67
Pointing Out SQL Queries From Text 4.67
Inducing Grammars with and for Neural Machine Translation 4.67
On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size 4.67
Gating out sensory noise in a spike-based Long Short-Term Memory network 4.67
Towards Building Affect sensitive Word Distributions 4.67
The Set Autoencoder: Unsupervised Representation Learning for Sets 4.67
Parametric Adversarial Divergences are Good Task Losses for Generative Modeling 4.67
Deep Learning is Robust to Massive Label Noise 4.67
Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner 4.67
Visualizing the Loss Landscape of Neural Nets 4.67
HyperNetworks with statistical filtering for defending adversarial examples 4.67
Topology Adaptive Graph Convolutional  Networks 4.67
Piecewise Linear Neural Networks verification: A comparative study 4.67
NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS 4.67
Joint autoencoders: a flexible meta-learning framework 4.67
Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning 4.67
Softmax Supervision with Isotropic Normalization 4.67
Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions 4.67
Learning to Infer Graphics Programs from Hand-Drawn Images 4.67
State Space LSTM Models with inference using Sequential Monte Carlo 4.67
Prototype Matching Networks for Large-Scale Multi-label  Genomic Sequence Classification 4.67
Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation 4.67
Combining Model-based and Model-free RL via Multi-step Control Variates 4.67
Depth separation and weight-width trade-offs for sigmoidal neural networks 4.67
BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK 4.67
Grouping-By-ID: Guarding Against Adversarial Domain Shifts 4.67
A comparison of second-order methods for deep convolutional neural networks 4.67
GENERATIVE LOW-SHOT NETWORK EXPANSION 4.67
Exploring the Space of Black-box Attacks on Deep Neural Networks 4.67
HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models 4.67
Learning what to learn in a neural program 4.67
Deep Epitome for Unravelling Generalized Hamming Network: A Fuzzy Logic Interpretation of Deep Learning 4.67
Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning 4.67
Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection 4.67
Cross-View Training for Semi-Supervised Learning 4.67
Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression 4.67
Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels 4.67
The Context-Aware Learner 4.67
PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training 4.67
Deep Function Machines: Generalized Neural Networks for Topological Layer Expression 4.67
Assessing Generalization Capability of Convolutional Neural Networks 4.67
Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling 4.67
Learning Deep ResNet Blocks Sequentially using Boosting Theory 4.50
Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger 4.50
Hybed: Hyperbolic Neural Graph Embedding 4.50
Characterizing Sparse Connectivity Patterns in Neural Networks 4.50
BinaryFlex: On-the-Fly Kernel Generation in Binary Convolutional Networks 4.33
Deep Boosting of Diverse Experts 4.33
The (Un)reliability of saliency methods 4.33
Relational Multi-Instance Learning for Concept Annotation from Medical Time Series 4.33
Training Deep AutoEncoders for Recommender Systems 4.33
A Tensor Analysis on Dense Connectivity via Convolutional Arithmetic Circuits 4.33
Information Theoretic Co-Training 4.33
Learning to Treat Sepsis with Multi-Output Gaussian Process Deep Recurrent Q-Networks 4.33
Image Segmentation by Iterative Inference from Conditional Score Estimation 4.33
A dynamic game approach to training robust deep policies 4.33
Autoregressive Generative Adversarial Networks 4.33
Dynamically Learning the Learning Rates:  Online Hyperparameter Optimization 4.33
UPS: optimizing Undirected Positive Sparse graph for neural graph filtering 4.33
Deep Active Learning over the Long Tail 4.33
Recurrent Relational Networks for complex relational reasoning 4.33
Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation 4.33
Time Limits in Reinforcement Learning 4.33
Adversarial Examples for Natural Language Classification Problems 4.33
Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks 4.33
TRL: Discriminative Hints for Scalable Reverse Curriculum Learning 4.33
A Neural Method for Goal-Oriented Dialog Systems to interact with Named Entities 4.33
AirNet: a machine learning dataset for air quality forecasting 4.33
Understanding Local Minima in Neural Networks by Loss Surface Decomposition 4.33
CNNs as Inverse Problem Solvers and Double Network Superresolution 4.33
Latent forward model for Real-time Strategy game planning with incomplete information 4.33
TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference 4.33
Learning to Generate Filters for Convolutional Neural Networks 4.33
LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES 4.33
Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit 4.33
Model Specialization for Inference Via End-to-End Distillation, Pruning, and Cascades 4.33
Extending the Framework of Equilibrium Propagation to General Dynamics 4.33
Towards Effective GANs for Data Distributions with Diverse Modes 4.33
Regularization for Deep Learning: A Taxonomy 4.33
A Neural-Symbolic Approach to Natural Language Tasks 4.33
Heterogeneous Bitwidth Binarization in Convolutional Neural Networks 4.33
Spectral Graph Wavelets for Structural Role Similarity in Networks 4.33
Diffusing Policies : Towards Wasserstein Policy Gradient Flows 4.33
Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures 4.33
LSD-Net: Look, Step and Detect for Joint Navigation and Multi-View Recognition with Deep Reinforcement Learning 4.33
Time-Dependent Representation for Neural Event Sequence Prediction 4.33
Training Neural Machines with Partial Traces 4.33
Online Hyper-Parameter Optimization 4.33
Discovery of Predictive Representations With a Network of General Value Functions 4.33
Multi-task Learning on MNIST Image Datasets 4.33
Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz 4.33
Achieving morphological agreement with Concorde 4.33
OMIE: The Online Mutual Information Estimator 4.33
DeepArchitect: Automatically Designing and Training Deep Architectures 4.33
Statestream: A toolbox to explore layerwise-parallel deep neural networks 4.33
Gated ConvNets for Letter-Based ASR 4.33
Influence-Directed Explanations for Deep Convolutional Networks 4.33
Word2net: Deep Representations of Language 4.33
Improving Conditional Sequence Generative Adversarial Networks by Evaluating at Every Generation Step 4.33
Learning to navigate by distilling visual information and natural language instructions 4.33
The Principle of Logit Separation 4.33
Image Transformer 4.33
Toward predictive machine learning for active vision 4.33
Sequence Transfer Learning for Neural Decoding 4.33
ShakeDrop regularization 4.33
A Goal-oriented Neural Conversation Model by Self-Play 4.33
Melody Generation for Pop Music via Word Representation of Musical Properties 4.33
Understanding GANs: the LQG Setting 4.33
Learning Generative Models with Locally Disentangled Latent Factors 4.33
Learning Deep Generative Models With Discrete Latent Variables 4.33
Distributed Fine-tuning of Language Models on Private Data 4.33
The Mutual Autoencoder: Controlling Information in Latent Code Representations 4.33
Do Deep Reinforcement Learning Algorithms really Learn to Navigate? 4.33
Improving image generative models with human interactions 4.33
3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY 4.33
Unsupervised Learning of Entailment-Vector Word Embeddings 4.33
Generalized Graph Embedding Models 4.33
CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior 4.33
A Classification-Based Perspective on GAN Distributions 4.33
Representing dynamically: An active process for describing sequential data 4.25
Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees 4.00
Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning 4.00
Tracking Loss: Converting Object Detector to Robust Visual Tracker 4.00
Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines 4.00
POLICY DRIVEN GENERATIVE ADVERSARIAL NETWORKS FOR ACCENTED SPEECH GENERATION 4.00
Learning to Select: Problem, Solution, and Applications 4.00
Continuous Propagation: Layer-Parallel Training 4.00
Discovering Order in Unordered Datasets: Generative Markov Networks 4.00
Learning Representations for Faster Similarity Search 4.00
Cluster-based Warm-Start Nets 4.00
What are image captions made of? 4.00
Now I Remember! Episodic Memory For Reinforcement Learning 4.00
Combination of Supervised and Reinforcement Learning For Vision-Based Autonomous Control 4.00
Post-training for Deep Learning 4.00
Adversarial Spheres 4.00
Learning Document Embeddings With CNNs 4.00
Embedding Deep Networks into Visual Explanations 4.00
Make SVM great again with Siamese kernel for  few-shot learning 4.00
Multi-Advisor Reinforcement Learning 4.00
Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study 4.00
GATED FAST WEIGHTS FOR ASSOCIATIVE RETRIEVAL 4.00
Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation 4.00
Parametric Manifold Learning Via Sparse Multidimensional Scaling 4.00
TCAV: Relative concept importance testing with Linear Concept Activation Vectors 4.00
Convolutional Mesh Autoencoders for 3D Face Representation 4.00
DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER 4.00
Directing Generative Networks with Weighted Maximum Mean Discrepancy 4.00
Graph2Seq: Scalable Learning Dynamics for Graphs 4.00
Discrete Wasserstein Generative Adversarial Networks (DWGAN) 4.00
Reward Design in Cooperative Multi-agent Reinforcement Learning for Packet Routing 4.00
Self-Supervised Learning of Object Motion Through Adversarial Video Prediction 4.00
Balanced and Deterministic Weight-sharing Helps Network Performance 4.00
Adaptive Weight Sparsity for Training Deep Neural Networks 4.00
Nearest Neighbour Radial Basis Function Solvers for Deep Neural Networks 4.00
Adversary A3C for Robust Reinforcement Learning 4.00
Deep Temporal Clustering: Fully unsupervised learning of time-domain features 4.00
Lifelong Word Embedding via Meta-Learning 4.00
Key Protected Classification for GAN Attack Resilient Collaborative Learning 4.00
Unsupervised Hierarchical Video Prediction 4.00
Clipping Free Attacks Against Neural Networks 4.00
Data augmentation instead of explicit regularization 4.00
Neural Networks for irregularly observed continuous-time Stochastic Processes 4.00
Transfer Learning on Manifolds via Learned Transport Operators 4.00
Manifold Assumption and Defenses Against Adversarial Perturbations 4.00
Siamese Survival Analysis 4.00
Learning Less-Overlapping Representations 4.00
LatentPoison -- Adversarial Attacks On The Latent Space 4.00
Improving generalization with Wasserstein regularization 4.00
Open Loop Hyperparameter Optimization and Determinantal Point Processes 4.00
Exploring Representation Methods for Sequence Labeling 4.00
Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling 4.00
Representing Entropy : A short proof of the equivalence between soft Q-learning and policy gradients 4.00
Generalization of Learning using Reservoir Computing 4.00
A Simple Fully Connected Network for Composing Word Embeddings from Characters 4.00
Towards Unsupervised Classification with Deep Generative Models 4.00
Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs) 4.00
Taking Apart Autoencoders: How do They Encode Geometric Shapes ? 4.00
Neural Tree Transducers for Tree to Tree Learning 4.00
Generative Adversarial Networks using Adaptive Convolution 4.00
Evolutionary Expectation Maximization 4.00
Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates 4.00
AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION 4.00
Accelerating Convolutional Neural Networks using Iterative Two-Pass Decomposition 4.00
ResBinNet: Residual Binary Neural Network 4.00
Reward Estimation via State Prediction 4.00
Learning a face space for experiments on human identity 4.00
Maintaining cooperation in complex social dilemmas using deep reinforcement learning 3.67
FastNorm: Improving Numerical Stability of Deep Network Training with Efficient Normalization 3.67
AANN: Absolute Artificial Neural Network 3.67
Simple Nearest Neighbor Policy Method for Continuous Control Tasks 3.67
LEAP: Learning Embeddings for Adaptive Pace 3.67
One-shot and few-shot learning of word embeddings 3.67
Graph Topological Features via GAN 3.67
Understanding Deep Learning Generalization by Maximum Entropy 3.67
Prediction Under Uncertainty with Error Encoding Networks 3.67
MULTI-MODAL GEOLOCATION ESTIMATION USING DEEP NEURAL NETWORKS 3.67
Structured Deep Factorization Machine: Towards General-Purpose Architectures 3.67
Cross-Corpus Training with TreeLSTM for the Extraction of Biomedical Relationships from Text 3.67
DEEP DENSITY NETWORKS AND UNCERTAINTY IN RECOMMENDER SYSTEMS 3.67
Video Action Segmentation with Hybrid Temporal Networks 3.67
Convolutional Normalizing Flows 3.67
Long Term Memory Network for Combinatorial Optimization Problems 3.67
When and where do feed-forward neural networks learn localist representations? 3.67
Modifying memories in a Recurrent Neural Network Unit 3.67
Learning objects from pixels 3.67
Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification 3.67
Semi-supervised Outlier Detection using Generative And Adversary Framework 3.67
THE LOCAL DIMENSION OF DEEP MANIFOLD 3.67
Faster Distributed Synchronous SGD with Weak Synchronization 3.67
On Characterizing the Capacity of Neural Networks Using Algebraic Topology 3.67
Empirical Investigation on Model Capacity and Generalization of Neural Networks for Text 3.67
Dependent Bidirectional RNN with Extended-long Short-term Memory 3.67
Evaluation of generative networks through their data augmentation capacity 3.67
Generative Models for Alignment and Data Efficiency in Language 3.67
XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings 3.67
Interpreting Deep Classification Models With Bayesian Inference 3.67
Understanding and Exploiting the Low-Rank Structure of Deep Networks 3.67
Neural Variational Sparse Topic Model 3.67
Convergence rate of sign stochastic gradient descent for non-convex functions 3.67
Tree2Tree Learning with Memory Unit 3.67
ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS 3.67
Autostacker: an Automatic Evolutionary Hierarchical  Machine Learning System 3.67
An Out-of-the-box Full-network Embedding for Convolutional Neural Networks 3.67
Bit-Regularized Optimization of Neural Nets 3.67
Multi-label Learning for Large Text Corpora using Latent Variable Model with Provable Gurantees 3.67
Label Embedding Network: Learning Label Representation for Soft Training of Deep Networks 3.67
Revisiting Knowledge Base Embedding as Tensor Decomposition 3.67
A Self-Organizing Memory Network 3.50
Associative Conversation Model: Generating Visual Information from Textual Information 3.33
Dense Recurrent Neural Network with Attention Gate 3.33
Do Convolutional Neural Networks act  as Compositional Nearest Neighbors? 3.33
Sparse Regularized Deep Neural Networks For Efficient Embedded Learning 3.33
Link Weight Prediction with Node Embeddings 3.33
Parametrizing filters of a CNN with a GAN 3.33
On the Construction and Evaluation of Color Invariant Networks 3.33
Noise-Based Regularizers for Recurrent Neural Networks 3.33
What cannot be learnt by Variational Autoencoder? 3.33
An inference-based policy gradient method for learning options 3.33
A Self-Training Method for Semi-Supervised GANs 3.33
Learning with Mental Imagery 3.33
Style Memory: Making a Classifier Network Generative 3.33
Estimation of cross-lingual news similarities using text-mining methods 3.33
Generative Discovery of Relational Medical Entity Pairs 3.33
EXPLORING NEURAL ARCHITECTURE SEARCH FOR LANGUAGE TASKS 3.33
Gaussian Prototypical Networks for Few-Shot Learning on Omniglot 3.33
Learning To Generate Reviews and Discovering Sentiment 3.33
Soft Value Iteration Networks for Planetary Rover Path Planning 3.33
DNN Model Compression Under Accuracy Constraints 3.33
A Deep Predictive Coding Network for Learning Latent Representations 3.33
Autonomous Vehicle Fleet Coordination With Deep Reinforcement Learning 3.33
Variable Activation Networks: A simple method to train deep feed-forward networks without skip-connections 3.33
Learning Topics using Semantic Locality 3.33
Anticipatory Asynchronous Advantage Actor-Critic (A4C): The power of Anticipation in Deep Reinforcement Learning 3.00
Lifelong Learning with Output Kernels 3.00
Sequential Coordination of Deep Models for Learning Visual Arithmetic 3.00
Zero-shot Cross Language Text Classification 3.00
Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification 3.00
Don't encrypt the data; just approximate the model \ Towards Secure Transaction and Fair Pricing of Training Data 3.00
Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games 3.00
Distributed non-parametric deep and wide networks 3.00
Data-efficient Deep Reinforcement Learning for Dexterous Manipulation 3.00
Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving 3.00
NOVEL RANKING BASED LEXICAL SIMILARITY MEASURE FOR WORD EMBEDDING 3.00
Spontaneous Symmetry Breaking in Deep Neural Networks 3.00
WHAT ARE GANS USEFUL FOR? 3.00
Complex- and Real-Valued Neural Network Architectures 3.00
Learning to Imagine Manipulation Goals for Robot Task Planning 3.00
TD Learning with Constrained Gradients 3.00
Improving Discriminator-Generator Balance in Generative Adversarial Networks 3.00
Improving Search Through A3C Reinforcement Learning Based Conversational Agent 3.00
Contextual memory bandit for pro-active dialog engagement 2.67
Simple Fast Convolutional Feature Learning 2.67
Clustering with Deep Learning: Taxonomy and New Methods 2.67
Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs) 2.67
A closer look at the word analogy problem 2.67
How do deep convolutional neural networks learn from raw audio waveforms? 2.67
DOUBLY STOCHASTIC ADVERSARIAL AUTOENCODER 2.67
On the Generalization Effects of DenseNet Model Structures  2.67
Self-Organization adds application robustness to deep learners 2.67
Improve Training Stability of Semi-supervised Generative Adversarial Networks with Collaborative Training 2.67
Lung Tumor Location and Identification with AlexNet and a Custom CNN 2.67
Recurrent Auto-Encoder Model for Multidimensional Time Series Representation 2.67
Learning to play slot cars and Atari 2600 games in just minutes 2.67
Exploring Sentence Vectors Through Automatic Summarization 2.33
TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING 2.33
DEMYSTIFYING WIDE NONLINEAR AUTO-ENCODERS: FAST SGD CONVERGENCE TOWARDS SPARSE REPRESENTATION FROM RANDOM INITIALIZATION 2.33
Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning 2.33
A novel method to determine the number of latent dimensions with SVD 2.00
APPLICATION OF DEEP CONVOLUTIONAL NEURAL NETWORK TO PREVENT ATM FRAUD BY FACIAL DISGUISE IDENTIFICATION 2.00
ENRICHMENT OF FEATURES FOR CLASSIFICATION USING AN OPTIMIZED LINEAR/NON-LINEAR COMBINATION OF INPUT FEATURES 2.00
Causal Generative Neural Networks 0.00

Process finished with exit code 0

